
2024.5.7 The text generated by LLMs has been updated and the remaining introductions will be updated later.


# CP-KGC
The paper is available at: [Can Text-based Knowledge Graph Completion Benefit From Zero-Shot Large Language Models?](https://arxiv.org/pdf/2310.08279.pdf) 

In this paper, we found that 
(1) We validated the effectiveness of CP-KGC using three publicly available datasets. CP-KGC enhances the semantic integrity of text and further improves the performance of text-based approaches.
(2) We demonstrated the effectiveness of quantized model (Qwen-7B-Chat-int4) by utilizing LLMs with various parameter scales. This enhances the feasibility of further research on the integration of KGC and LLMs.
(3) CP-KGC's contextual constraints strategy accurately identifies polysemous entities in datasets. Additionally, this strategy can also enhance the stability of text generation by LLMs.

![Alt text](./model.png)
CP-KGC semantic enhancement framework.


# Requirements

* python>=3.8
* torch>=1.8 (for mixed precision training)
* transformers>=4.15

All experiments are run with 1 A800(80GB) GPU.

If you want to reproduce our best experimental results, you need to download the model weights [here](https://drive.google.com/drive/my-drive), and replace the file path.

The graphics required to reproduce the experiment is about 78GB.

CP-KGC used [SimKGC](https://github.com/intfloat/SimKGC) as the basic model in the paper. 

```
pip install transformers
```

Just replace the generated latest data of FB15K-237 and WN18RR in SimKGC.


* In FB15k-237, you need to replace FB15k_mid2description.txt with FB15k_mid2description_qwen-7-int4_final.txt.
* In WN18RRï¼Œyou need to replace wordnet-mlj12-definitions.txt with wordnet-mlj12-definitions_add_examples_gpt4_final.txt.



For filtering synonyms, please refer to the **Synonyms_WN18RR** folder.


If you **don't have the computing resources**, you can use the **Qwen-7B-Chat** and **LLaMA2-7B/13B-Chat** inference tests [here](https://modelscope.cn/topic/dfefe5be778b49fba8c44646023b57ba/pub/summary). ModelScope provides sufficient computing resources for inference testing of the 13B model. You can also use quantized models for inference.

# Contribute together

Apart from SimKGC, we have also tested the KG-S2S model, all of which are text-based knowledge graph completion models.

Would you like to proceed with further testing using a One-Shot or Few-Shot approach, or experiment with prompts that you find effective?

General-domain testing is meant to demonstrate the feasibility of this approach, while further exploration would require experimentation on domain-specific data. Considering the distribution of model training data, fine-tuning the model is necessary for vertical domains.

|       | | WN18RR|     |      |  |FB15K237 |    |     |
|-------|--------|-----|-----|------|----------|----|----|-----|
|       | MRR    | H@1 | H@3 | H@10 | MRR      | H@1| H@3| H@10|
|SimKGC |66.6|58.7|71.7|80.0|33.6|24.9|36.2|51.1|
|SimKGC+CP-KGC|67.3|59.9|72.1|80.4|33.8|25.1|36.5|51.6|
|KG-S2S |57.4|53.1|59.5|66.1|32.6|24.9|35.8|48.7|
|KG-S2S+CP-KGC |57.9|53.3|60.3|66.7|32.8|25.1|36.2|50.1|




# Citation
If you find our paper or code repository helpful, please consider citing as follows:
```
@article{yang2023cp,
  title={CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large Language Models},
  author={Yang, Rui and Fang, Li and Zhou, Yi},
  journal={arXiv preprint arXiv:2310.08279},
  year={2023}
}
```
